* Concat: 

        self.ntags_pos = ntags_pos
        self.ntags_ner = ntags_ner
        self.embeddings = embeddings
        self.utils = utils
        self.train_embeddings = False
        self.nepochs =50
        self.keep_prob = 0.5
        self.batch_size = 512
        self.lr_method = "adam"
        self.learning_rate = 0.001
        self.lr_decay = 0.9
        self.clip = -1  # if negative, no clipping
        self.nepoch_no_imprv = 5
        # model hyperparameters
        self.hidden_size_lstm = 300  # lstm on word embeddings

Pos acc: 95.10, f1: 0.95
Ner acc: 98.26, f1: 0.98



-------------------------------------------------
        self.ntags_pos = ntags_pos
        self.ntags_ner = ntags_ner
        self.embeddings = embeddings
        self.utils = utils
        self.train_embeddings = False
        self.nepochs =50
        self.keep_prob = 0.5
        self.batch_size = 512
        self.lr_method = "sgd"
        self.learning_rate = 0.001
        self.lr_decay = 0.9
        self.clip = -1  # if negative, no clipping
        self.nepoch_no_imprv = 5
        # model hyperparameters
        self.hidden_size_lstm = 300  # lstm on word embeddings
        

Pos: acc 10.90 - Pos: f1 0.11
Ner: acc 83.23 - Ner: f1 0.83


------------------------------------------------------
        self.ntags_pos = ntags_pos
        self.ntags_ner = ntags_ner
        self.embeddings = embeddings
        self.utils = utils
        self.train_embeddings = False
        self.nepochs =50
        self.keep_prob = 0.5
        self.batch_size = 512
        self.lr_method = "rmsprop"
        self.learning_rate = 0.001
        self.lr_decay = 0.9
        self.clip = -1  # if negative, no clipping
        self.nepoch_no_imprv = 5
        # model hyperparameters
        self.hidden_size_lstm = 300  # lstm on word embeddings



Pos: f1 0.95 - Pos: acc 94.97
Ner: f1 0.98 - Ner: acc 98.25

--------------------------------------------------------



        self.ntags_pos = ntags_pos
        self.ntags_ner = ntags_ner
        self.embeddings = embeddings
        self.utils = utils
        self.train_embeddings = False
        self.nepochs =50
        self.keep_prob = 0.5
        self.batch_size = 256
        self.lr_method = "adam"
        self.learning_rate = 0.001
        self.lr_decay = 0.9
        self.clip = -1  # if negative, no clipping
        self.nepoch_no_imprv = 5
        # model hyperparameters
        self.hidden_size_lstm = 600  # lstm on word embeddings

acc 96.57 - Pos: f1 0.97
Ner: acc 99.50 - Ner: f1 0.99
------------------------------------------------------------



* add

        self.ntags_pos = ntags_pos
        self.ntags_ner = ntags_ner
        self.embeddings = embeddings
        self.utils = utils
        self.train_embeddings = False
        self.nepochs =50
        self.keep_prob = 0.5
        self.batch_size = 256
        self.lr_method = "adam"
        self.learning_rate = 0.001
        self.lr_decay = 0.9
        self.clip = -1  # if negative, no clipping
        self.nepoch_no_imprv = 5
        # model hyperparameters
        self.hidden_size_lstm = 600  # lstm on word embeddings
        


Pos: f1 0.97 - Pos: acc 96.69
Ner: f1 1.00 - Ner: acc 99.55

-----------------------------------------------------------


        self.ntags_pos = ntags_pos
        self.ntags_ner = ntags_ner
        self.embeddings = embeddings
        self.utils = utils
        self.train_embeddings = False
        self.nepochs =50
        self.keep_prob = 0.5
        self.batch_size = 256
        self.lr_method = "adam"
        self.learning_rate = 0.01
        self.lr_decay = 0.5
        self.clip = 1  # if negative, no clipping
        self.nepoch_no_imprv = 5
        # model hyperparameters
        self.hidden_size_lstm = 600  # lstm on word embeddings
        

Pos: acc 96.84 - Pos: f1 0.97
Ner: acc 99.34 - Ner: f1 0.99

Loss increasing after 12 epochs, best loss: 0.1249  



-----------------------------------------------


        # training
        self.ntags_pos = ntags_pos
        self.ntags_ner = ntags_ner
        self.embeddings = embeddings
        self.utils = utils
        self.train_embeddings = False
        self.nepochs =50
        self.keep_prob = 0.2
        self.batch_size = 256
        self.lr_method = "adam"
        self.learning_rate = 0.001
        self.lr_decay = 0.9
        self.clip = 1  # if negative, no clipping
        self.nepoch_no_imprv = 5
        # model hyperparameters
        self.hidden_size_lstm = 600  # lstm on word embeddings

Pos: acc 92.67 - Pos: f1 0.93
Ner: acc 97.38 - Ner: f1 0.97


---------------------------------


WTF!!!

        self.ntags_pos = ntags_pos
        self.ntags_ner = ntags_ner
        self.embeddings = embeddings
        self.utils = utils
        self.train_embeddings = False
        self.nepochs =50
        self.keep_prob = 0.8
        self.batch_size = 256
        self.lr_method = "adam"
        self.learning_rate = 0.001
        self.lr_decay = 0.9
        self.clip = 1  # if negative, no clipping
        self.nepoch_no_imprv = 5
        # model hyperparameters
        self.hidden_size_lstm = 600  # lstm on word embeddings
        

Pos: f1 0.97 - Pos: acc 97.11
Ner: f1 1.00 - Ner: acc 99.96



* Unidirectional


        # training
        self.ntags_pos = ntags_pos
        self.ntags_ner = ntags_ner
        self.embeddings = embeddings
        self.utils = utils
        self.train_embeddings = False
        self.nepochs =50
        self.keep_prob = 0.8
        self.batch_size = 256
        self.lr_method = "adam"
        self.learning_rate = 0.001
        self.lr_decay = 0.9
        self.clip = 1  # if negative, no clipping
        self.nepoch_no_imprv = 5
        # model hyperparameters
        self.hidden_size_lstm = 600  # lstm on word embeddings
        


Pos: acc 96.01 - Pos: f1 0.96
Ner: acc 99.35 - Ner: f1 0.99
